p8105_hw2_bk2959
================
Stella Koo
2024-09-24

## Problem 1

### Data Cleaning

``` r
library(tidyverse)
options(scipen = 999)
```

``` r
nyc_subway_df = 
  read_csv("./nyc_transit_subway.csv", na = c("")) |>
  janitor::clean_names() |>
  select(line:route11, entry, vending, entrance_type, ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE),
         route8 = as.character(route8),
         route9 = as.character(route9),
         route10 = as.character(route10),
         route11 = as.character(route11)
  )

nyc_subway_df
```

    ## # A tibble: 1,868 × 19
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <chr>, route9 <chr>, route10 <chr>, route11 <chr>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

The dataset contains information about subway stations in NYC with a
wide range of variables: line, station name, station latitude and
longitude, routes served, entry, vending, entrance type, and ADA
compliance. It has 1868 rows and 19 columns.

These data are not tidy because there are many repeating observations:
there are multiple entries for the same station (e.g., 25th St and 36th
St) with different entrance locations, which creates redundancy. The
dataset also has multiple route columns (Route1, Route2, etc.), but not
all rows have data for every route. This results in a wide data
structure where many cells are empty.

- **Data Import:** `read_csv()` was used to import the CSV file into R
  as a tibble.
- **Column Cleaning:** `janitor::clean_names()` was applied to clean and
  standardize the column names.
- **Column Selection:** `select()` was utilized to choose specific
  columns from the dataset and organize them in the desired order.
- **Data Transformation:** `mutate()` and `ifelse()` were used to
  convert `entry` variable from character type (YES and NO) to logical
  variable (TRUE and FALSE). The columns `route8` to `route11` were
  converted to character variable to maintain consistency with other
  route columns.

### Answers to Questions

``` r
distinct_stations = distinct(nyc_subway_df, station_name, ada)
total_distinct_stations = nrow(distinct_stations)
ada_compliant_stations = nrow(filter(distinct_stations, ada == TRUE))

no_vending = filter(nyc_subway_df, vending == "NO")
total_no_vending = nrow(no_vending)
allow_entrance = nrow(filter(no_vending, entry == TRUE))
proportion_allow_entrance = round(allow_entrance/total_no_vending, 3)
```

There are 381 distinct stations. 73 stations are ADA compliant. The
proportion of station entrances without vending that allow entrance is
0.377.

##### Reformatted data: route number and route name as distinct variables

``` r
nyc_subway_tidy = 
  pivot_longer(
    nyc_subway_df,
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  ) |>
  drop_na(route_name)

nyc_subway_tidy
```

    ## # A tibble: 4,270 × 10
    ##    line     station_name station_latitude station_longitude entry vending
    ##    <chr>    <chr>                   <dbl>             <dbl> <lgl> <chr>  
    ##  1 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  2 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  3 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  4 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  5 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  6 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  7 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  8 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  9 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## 10 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## # ℹ 4,260 more rows
    ## # ℹ 4 more variables: entrance_type <chr>, ada <lgl>, route_number <chr>,
    ## #   route_name <chr>

``` r
a_train_stations = 
  nyc_subway_tidy |>
  filter(route_name == "A") |>
  distinct(station_name, ada)

total_a_train = nrow(a_train_stations)

a_train_ada = 
  a_train_stations |>
  filter(ada == TRUE) |>
  nrow()
```

The A train serves 57 different stations. Of the stations that serve the
A train, 16 are ADA compliant.

## Problem 2

### Data Cleaning

``` r
library(readxl)

mr_trash_wheel = 
  read_excel("./trash_wheel_data.xlsx", 
             sheet = "Mr. Trash Wheel", 
             range = "A2:N653",
             na = c("")) |>
  janitor::clean_names() |>
  mutate(sports_balls = as.integer(round(sports_balls, 0)),
         trash_wheel_name = "Mr. Trash Wheel",
         year = as.integer(year))
```

``` r
professor_trash_wheel = 
  read_excel("./trash_wheel_data.xlsx", 
             sheet = "Professor Trash Wheel", 
             range = "A2:M120",
             na = c("")) |>
  janitor::clean_names() |>
  mutate(trash_wheel_name = "Professor Trash Wheel",
         year = as.integer(year))
```

``` r
gwynnda_trash_wheel = 
  read_excel("./trash_wheel_data.xlsx", 
             sheet = "Gwynnda Trash Wheel", 
             range = "A2:L265",
             na = c("")) |>
  janitor::clean_names() |>
  mutate(trash_wheel_name = "Gwynnda Trash Wheel",
         year = as.integer(year))
```

### Combined Dataset

``` r
combined_trash_wheel = 
  bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda_trash_wheel) |>
  select(trash_wheel_name, everything())

combined_trash_wheel
```

    ## # A tibble: 1,032 × 15
    ##    trash_wheel_name dumpster month  year date                weight_tons
    ##    <chr>               <dbl> <chr> <int> <dttm>                    <dbl>
    ##  1 Mr. Trash Wheel         1 May    2014 2014-05-16 00:00:00        4.31
    ##  2 Mr. Trash Wheel         2 May    2014 2014-05-16 00:00:00        2.74
    ##  3 Mr. Trash Wheel         3 May    2014 2014-05-16 00:00:00        3.45
    ##  4 Mr. Trash Wheel         4 May    2014 2014-05-17 00:00:00        3.1 
    ##  5 Mr. Trash Wheel         5 May    2014 2014-05-17 00:00:00        4.06
    ##  6 Mr. Trash Wheel         6 May    2014 2014-05-20 00:00:00        2.71
    ##  7 Mr. Trash Wheel         7 May    2014 2014-05-21 00:00:00        1.91
    ##  8 Mr. Trash Wheel         8 May    2014 2014-05-28 00:00:00        3.7 
    ##  9 Mr. Trash Wheel         9 June   2014 2014-06-05 00:00:00        2.52
    ## 10 Mr. Trash Wheel        10 June   2014 2014-06-11 00:00:00        3.76
    ## # ℹ 1,022 more rows
    ## # ℹ 9 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
    ## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
    ## #   plastic_bags <dbl>, wrappers <dbl>, sports_balls <int>, homes_powered <dbl>

This dataset documents the collections from various trash wheels
(`Mr. Trash Wheel`, `Professor Trash Wheel`, and `Gwynnda`) over
distinct time periods. It consists of 1032 x 15 and includes
comprehensive quantitative data that highlights:

- **Weight and Volume (`weight_tons` and `volume_cubic_yards`):**
  Detailed metrics on the total weight and volume of waste collected by
  each trash wheel.
- **Composition Analysis (`plastic_bottles`, `polystyrene`, etc):** A
  breakdown of the materials found in each dumpster, including specific
  categories such as plastic bottles, polystyrene, cigarettes, and other
  waste types.
- **Energy Metrics (`homes_powered`):** It also includes information on
  the energy generated from the waste collected, measured in kilowatts
  (kW).

### Total Weight of Trash

``` r
total_weight_professor = 
  professor_trash_wheel |>
  pull(weight_tons) |>
  sum(na.rm = TRUE)

june2022_cigarette_gwynnda = 
  filter(gwynnda_trash_wheel, month == "June", year == 2022) |>
  pull(cigarette_butts) |>
  sum(na.rm = TRUE)
```

The total weight of trash collected by Professor Trash Wheel is 246.74.
The total number of cigarette butts collected by Gwynnda in June of 2022
is 18120.

## Problem 3

### Data Cleaning

The datasets `bakers.csv`, `bakes.csv`, and `results.csv` were imported
into this R project using the `read_csv()` function. Column names were
standardized across all datasets using `janitor::clean_names()` to
ensure consistency. Several columns were also reorganized and renamed to
facilitate seamless joins during the data cleaning process:

- `bakers_df`: The bakers’ names were split into separate `first_name`
  and `last_name` columns to align with other datasets, which only
  contain first names. Additionally, the age and occupation columns were
  renamed to avoid redundancy.
- `bakers_df` and `results_df`: The column containing the bakers’ names
  was renamed to `first_name` to maintain uniformity across all three
  datasets.

``` r
bakers_df = 
  read_csv("./gbb_datasets/bakers.csv", na = c("NA")) |>
  janitor::clean_names() |>
  separate(baker_name, into = c("first_name", "last_name"), sep = " ") |>
  rename(age = baker_age, occupation = baker_occupation)

bakes_df = 
  read_csv("./gbb_datasets/bakes.csv", na = c("NA")) |>
  janitor::clean_names() |>
  rename(first_name = baker) 

results_df = 
  read_csv("./gbb_datasets/results.csv", na = c("NA"), skip = 2) |>
  janitor::clean_names() |>
  rename(first_name = baker) 
```

### Joining Datasets

Before merging the datasets, it is essential to verify both completeness
and accuracy across them in order to determine the appropriate type of
join and ensure that the final dataset retains only the relevant data.
The `anti_join()` function was employed to identify discrepancies
between datasets:

- `missing_bakers`: Identifies bakes listed in `bakes_df` that are not
  linked to any known baker in `bakers_df`. This check helps highlight
  instances where bakes are recorded without a corresponding baker.
- `missing_bakes`: Displays entries from `results_df` that lack a
  corresponding bake in `bakes_df`. This check is useful for detecting
  episodes where results were recorded, but no bakes were logged in the
  dataset.

``` r
missing_bakers = anti_join(bakes_df, bakers_df, by = c("first_name", "series"))
missing_bakes = anti_join(results_df, bakes_df, by = c("first_name", "series", "episode"))

missing_bakers 
```

    ## # A tibble: 8 × 5
    ##   series episode first_name signature_bake                          show_stopper
    ##    <dbl>   <dbl> <chr>      <chr>                                   <chr>       
    ## 1      2       1 "\"Jo\""   Chocolate Orange CupcakesOrange and Ca… Chocolate a…
    ## 2      2       2 "\"Jo\""   Caramelised Onion, Gruyere and Thyme Q… Raspberry a…
    ## 3      2       3 "\"Jo\""   Stromboli flavored with Mozzarella, Ha… Unknown     
    ## 4      2       4 "\"Jo\""   Lavender Biscuits                       Blueberry M…
    ## 5      2       5 "\"Jo\""   Salmon and Asparagus Pie                Apple and R…
    ## 6      2       6 "\"Jo\""   Rum and Raisin Baked Cheesecake         Limoncello …
    ## 7      2       7 "\"Jo\""   Raspberry & Strawberry Mousse Cake      Pain Aux Ra…
    ## 8      2       8 "\"Jo\""   Raspberry and Blueberry Mille Feuille   Mini Victor…

``` r
missing_bakes
```

    ## # A tibble: 596 × 5
    ##    series episode first_name technical result
    ##     <dbl>   <dbl> <chr>          <dbl> <chr> 
    ##  1      1       2 Lea               NA <NA>  
    ##  2      1       2 Mark              NA <NA>  
    ##  3      1       3 Annetha           NA <NA>  
    ##  4      1       3 Lea               NA <NA>  
    ##  5      1       3 Louise            NA <NA>  
    ##  6      1       3 Mark              NA <NA>  
    ##  7      1       4 Annetha           NA <NA>  
    ##  8      1       4 Jonathan          NA <NA>  
    ##  9      1       4 Lea               NA <NA>  
    ## 10      1       4 Louise            NA <NA>  
    ## # ℹ 586 more rows

The previous checks reveal several instances of missing data across the
datasets. The choice of join depends on the goals of the analysis, as
different join types will retain or exclude specific data. For this
assignment, I opted to use `full_join()` to merge all datasets, ensuring
that all information is included, even when some data is missing. For
example, if a baker is marked as STAR BAKER in an episode but their bake
information is absent, using `full_join()` guarantees that this result
is still captured, providing a comprehensive view of the available data.

``` r
bake_off_df = bakers_df |>
  full_join(bakes_df, by = c("first_name", "series")) |>
  full_join(results_df, by = c("first_name", "series", "episode")) |>
  select(series, episode, first_name, last_name, everything()) |>
  arrange(series, episode)

write_csv(bake_off_df, "bake_off_df.csv")
```

The final dataset consists of 1170 rows and 11 columns, providing a
comprehensive overview of the baking results, including all bakers,
bakes, and their corresponding results. While some missing values
remain, the dataset is structured to support further analysis.

### STAR BAKER and WINNERS

The table below presents the STAR BAKER or WINNER for each episode in
Seasons 5 through 10. From Seasons 5 to 8, all relevant data for the
selected columns are consistently recorded. However, there are
significant gaps in data for Seasons 9 and 10, particularly for columns
such as `last_name`, `age`, `occupation`, and `signature_bake`. This
suggests that data tracking has been unsuccessful since Season 9.

Many names appear repeatedly as STAR BAKER across multiple episodes,
indicating that these individuals have won multiple times. Notably, in
most seasons, these bakers have also been crowned the WINNER in the
final episode (episode 10). However, there are instances where the
WINNER was an unexpected choice. For example, in Season 10, while Steph
was recognized as STAR BAKER in several episodes, David was declared the
WINNER despite not having been a STAR BAKER in any of the previous
episodes.

``` r
bake_off_winners = bake_off_df |>
  filter(series >= 5, result == "STAR BAKER" | result == "WINNER") |>
  select(series:first_name, last_name, age, occupation, signature_bake, result)

bake_off_winners
```

    ## # A tibble: 60 × 8
    ##    series episode first_name last_name     age occupation  signature_bake result
    ##     <dbl>   <dbl> <chr>      <chr>       <dbl> <chr>       <chr>          <chr> 
    ##  1      5       1 Nancy      Birtwhistle    60 Retired Pr… Coffee and Ha… STAR …
    ##  2      5       2 Richard    Burr           38 Builder     Rosemary Seed… STAR …
    ##  3      5       3 Luis       Troyano        42 Graphic De… Opposites Att… STAR …
    ##  4      5       4 Richard    Burr           38 Builder     Black Forest … STAR …
    ##  5      5       5 Kate       Henry          41 Furniture … Rhubarb and C… STAR …
    ##  6      5       6 Chetna     Makan          35 Fashion De… Orange Savari… STAR …
    ##  7      5       7 Richard    Burr           38 Builder     Minted Lamb P… STAR …
    ##  8      5       8 Richard    Burr           38 Builder     Fruit Swedish… STAR …
    ##  9      5       9 Richard    Burr           38 Builder     Rose and Pist… STAR …
    ## 10      5      10 Nancy      Birtwhistle    60 Retired Pr… Apple and Lem… WINNER
    ## # ℹ 50 more rows

### Viewership Dataset

``` r
viewers_df =
  read_csv("./gbb_datasets/viewers.csv", na = c("NA"), show_col_types = FALSE) |> 
  janitor::clean_names()

head(viewers_df, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

The average viewership in Season 1 is 2.77 and in Season 5 is 10.04.
